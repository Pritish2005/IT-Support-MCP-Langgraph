{"cells": [{"cell_type": "code", "metadata": {}, "source": ["import asyncio\n", "from typing import List, TypedDict\n", "from langchain_mcp_adapters.client import MultiServerMCPClient\n", "from langchain_core.messages import HumanMessage\n", "from langgraph.graph import StateGraph, START, END\n", "from langgraph.prebuilt import create_react_agent\n", "from langchain_groq import ChatGroq\n", "from langgraph.graph.graph import MermaidDrawMethod\n", "\n", "class WorkflowState(TypedDict, total=False):\n", "    query: str\n", "    response: List\n", "    was_helpful: str\n", "    wants_escalation: str\n", "\n", "async def main():\n", "    client = MultiServerMCPClient(\n", "        {\n", "            "math": {\n", "                "command": "python",\n", "                "args": ["C:/Users/priti/OneDrive/Desktop/Hitachi/mcp_langgraph_integration/new/servers/math_server_mcp.py"],\n", "                "transport": "stdio",\n", "            },\n", "            "document_reader":{\n", "                "command": "python",\n", "                "args": ["C:/Users/priti/OneDrive/Desktop/Hitachi/mcp_langgraph_integration/new/servers/document_reader_tool.py"],\n", "                "transport":"stdio"\n", "            }\n", "        }\n", "    )\n", "    tools = await client.get_tools()\n", "   \n", "    agent = create_react_agent(\n", "        ChatGroq(groq_api_key="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", model_name="gemma2-9b-it"),\n", "        tools=tools,\n", "    )\n", "\n", "    async def handle_query(state):\n", "        response = await agent.ainvoke({"messages": [HumanMessage(content=state["query"])]})\n", "        state["response"] = response["messages"]\n", "        return state\n", "    \n", "    async def ask_was_helpful(state):\n", "        print("\\nAgent Response:")\n", "        for msg in state["response"]:\n", "            print("-", msg.content)\n", "        answer = input("\\nWas this helpful? (yes/no): ").strip().lower()\n", "        state["was_helpful"] = answer\n", "        return state\n", "    \n", "    async def ask_escalate(state):\n", "        if state["was_helpful"] == "yes":\n", "            return {"next": "end"}\n", "        answer = input("Would you like to escalate this to human IT support? (yes/no): ").strip().lower()\n", "        state["wants_escalation"] = answer\n", "        return {"next": "escalate" if answer == "yes" else "end"}\n", "    \n", "    async def escalate_to_gmail(state):\n", "        print("Sending email to IT support...")\n", "        return {"next": "end"}\n", "    \n", "    async def end_state(state):\n", "        print("Conversation ended.")\n", "        return state\n", "    \n", "    workflow = StateGraph(WorkflowState)\n", "    workflow.add_node("handle_query", handle_query)\n", "    workflow.add_node("ask_was_helpful", ask_was_helpful)\n", "    workflow.add_node("ask_escalate", ask_escalate)\n", "    workflow.add_node("escalate", escalate_to_gmail)\n", "    workflow.add_node("end", end_state)\n", "\n", "    workflow.set_entry_point("handle_query")\n", "    workflow.add_edge("handle_query", "ask_was_helpful")\n", "    workflow.add_edge("ask_was_helpful", "ask_escalate")\n", "    workflow.add_conditional_edges("ask_escalate", lambda state: state["next"], {\n", "        "escalate": "escalate",\n", "        "end": END\n", "    })\n", "    workflow.add_edge("escalate", "end")\n", "\n", "    graph = workflow.compile()\n", "    with open("workflow_graph.png", "wb") as f:\n", "        f.write(graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER))\n", "    print("Workflow graph saved to \"workflow_graph.png\".")\n", "\n", "    # Uncomment to run the workflow\n", "    # initial_state = {"query": "how do I fix my hardware issue"}\n", "    # await graph.ainvoke(initial_state)\n", "\n", "if __name__ == "__main__":\n", "    asyncio.run(main())"], "execution_count": None, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 2}